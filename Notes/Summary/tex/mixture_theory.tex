\section{Mixture model}

\subsection{Location mixture}
The model is the following
\begin{equation*}
    y_{it}=\theta_i+\epsilon_{it} \quad \text{where} \quad \epsilon_{it}\sim N(0,1)
\end{equation*}
Thus \begin{equation*}
    \hat{\theta}_i=\frac{1}{T_i}\sum y_{it} \sim N(\theta_i,1/T_i)
\end{equation*}
The likelihood function $L(F|y)$ (optimizing over distribution function $F$ given the observed $y$) is
\begin{equation*}
    \begin{split}
        L(F|y)&=\prod_{i=1}^N\int \prod_{1}^{T_i} \phi(y_{it}-\theta_i)dF(\theta_i)
    \end{split}
\end{equation*}
Instead of focusing on each observation $y_{it}$ we can also focus on the mean $\hat{\theta}_i=\bar{y}_i$.
If we utilize $\hat{\theta}_i-\theta_i \sim N(0,1/T_i)$, we can write the likelihood function as \begin{align*}
    L(F|y) & =\prod_{i=1}^N\int
    \phi((\hat{\theta}_i-\theta_i)\sqrt{T_i})\sqrt(T_i)dF \\
    l(F|y) & =\sum_{i=1}^N\log\int
    \phi((\hat{\theta}_i-\theta_i)\sqrt{T_i})\sqrt(T_i)dF
\end{align*}
Optimizing over all possible function $F$ neccesitates some kind of discrete approximation. The most common one is the grid approximation. We can also use the EM algorithm to optimize the likelihood function.
Let $f_j$ approximate the value of $d F$ on the grid
\begin{equation*}
    \max_f \set{ \sum_{i=1}^N\log g_i \big| g=Af, \sum_j f_j\Delta_j =1, f\ge 0}
\end{equation*}

\begin{remark}
    For a reader not as versed in mathematics as she should be. $A_{i*}f = \sum \sqrt{T_i}\phi((\hat{\theta}_i-\theta_j)\sqrt{T_i})f_j\Delta_j$.
    We use $\sum f_j\Delta_j$ to approximate the integral $\int dF$ as one can imagine.
\end{remark}
This a convex objective function subject to linear equality  and inequality constraints. The EM algorithm is a natural choice to optimize this function. The E-step is to calculate the expectation of the log-likelihood function given the observed data and the current estimate of the parameter. The M-step is to maximize the expectation of the log-likelihood function with respect to the parameter. The algorithm iterates between these two steps until convergence.
Often, the dual formulation of a convex objective is more efficient than the primal.
\begin{equation*}
    \max_f \set{ \sum_{i=1}^N\log (v_i)|A^T v = n 1_p, v\ge 0}
\end{equation*}
\begin{question}
    Derive on your own for practice
\end{question}
\begin{sol}
    We define the multiplier $\mu_1', \mu_2, \lambda'$ for the two equality and one inequality contstraints.
    The dual objective function is \begin{equation*}
        \min_{f,g} \set{-\sum_{i=1}^N\log g_i + \mu_1'(g-Af) + \mu_2(\sum_j f_j\Delta_j-1) - \lambda' f}
    \end{equation*}
    Minimize over each $g_i$ gives the condition \begin{equation*}
        \frac{1}{g_i}=\mu_{1i}
    \end{equation*}
    Minimize over each $f_i$ gives the condition \begin{equation*}
        -(\mu_1'A)_j+\mu_2-\lambda_j=0
    \end{equation*}
    Therefore the objective is \begin{equation*}
        \begin{array}{ll}
                              & \set{\sum \log \mu_{1i} + n - \mu_2 } \\
            \text{subject to} & -\mu_1'A+\mu_2 1_p-\lambda=0          \\
                              & \lambda\ge 0
        \end{array}
    \end{equation*} Thus the dual problem is \begin{equation*}
        \begin{array}{ll}
            \text{maximize}_{\mu_1',\mu_2} & \set{\sum \log \mu_{1i}+n-\mu_2} \\
            \text{subject to}              & \mu_1'A\le \mu_2 1_p             \\
                                           & \mu_1\ge 0
        \end{array}
    \end{equation*}
\end{sol}
\subsection{Scale mixture}
The model is the following:
\begin{equation*}
    y_{it}=\sigma_i\epsilon_{it} \quad \text{where} \quad \epsilon_{it}\sim N(0,1)
\end{equation*}
Similarly \begin{equation*}
    s_i=\hat{\sigma}_i^2=\frac{1}{m_i}\sum_{t=1}^{T_i}y_{it}^2
\end{equation*}
But what's the distribution of $\hat{\sigma}_i$ (which is not so obvious relative to $\hat{\theta}_i$)?
Well, $\frac{\sum y_{it}^2}{\sigma_i^2}$ follows a Gamma distribution with shape parameter $r_i=(T_i)/2$ and scale parameter $s_i=\sigma_i^2/r_i=2\sigma_i^2/T_i$.
\begin{question}
    Why is it gamma distribution?
\end{question}
\begin{sol}
    The sum of k independent standard normal variable  $X$ follows a $\chi^2(k)$ distribution. \\
    The mean of n independent $\chi^2(k)$ distribution variables $K$ follows a gamma distribution $\gamma(nk/2,2/n)$.\\
    The equivalence lies in \href{https://en.wikipedia.org/wiki/Gamma_distribution}{here}: if $X \sim \gamma(v/2, 2)$ (in the shape-scale parametrization), then $X$ is identical to $\chi^2(v)$, the chi-squared distribution with $v$ degrees of freedom. Conversely, if $Q\sim \chi^2(v)$ and c is a positive constant, then $cQ ~ \gamma(v/2, 2c)$.
\end{sol}
\begin{remark}
    The Gamma distribution $\gamma(k, \theta)$ (shape, scale) has the following distribution function \begin{equation*}
        f(x|k,\theta)=\frac{1}{\Gamma(k)\theta^k}x^{k-1}e^{-x/\theta} \quad F(x|k,\theta)=\frac{1}{\Gamma(k)}\gamma(k,x/\theta)
    \end{equation*} where $\Gamma(z)=\int_0^{\infty}t^{z-1}e^{-t}dt$.
\end{remark}
Thus the likelihood function is \begin{align*}
    L(F|y) & =\prod_{i=1}^N\int \gamma(s_i|r_i,\sigma_i)dF(\sigma_i)    \\
    l(F|y) & =\sum_{i=1}^N\log\int \gamma(s_i|r_i,\sigma_i)dF(\sigma_i)
\end{align*}
which we can proceed just as in the location mixture case.
\subsection{Location-scale mixture (independent)}
The model is \begin{equation*}
    y_{it}=\theta_i+\sigma_i\epsilon_{it} \quad \text{where} \quad \epsilon_{it}\sim N(0,1)
\end{equation*}
The sufficient statistics $\hat{\theta}_i$ and $\hat{\sigma}_i$ are \begin{align*}
    \hat{\theta}_i & =\frac{1}{T_i}\sum_{t=1}^{T_i}y_{it} \sim N(\theta_i, \sigma_i^2/T_i)                                       \\
    \hat{\sigma}_i & =\frac{1}{T_i}\sum_{t=1}^{T_i}(y_{it}-\hat{\theta}_i)^2\sim \gamma(s_i | \alpha= r_i, \beta=\sigma_i^2/r_i)\end{align*}
Just as in the previous two cases, we can write the (log) likelihood as \begin{equation*}
    l(G_{\theta},F_{\sigma}|y)=\sum_{i=1}^N\log\int \int \bra{\phi((\hat{\theta}_i-\theta_i)\sqrt{T_i})\sqrt{T_i}} \bra{\gamma(s_i|r_i,\sigma_i)} dG_{\theta}(\theta_i) dF_{\sigma}(\sigma_i)
\end{equation*}
For \textbf{estimation}, we can first solve for $\hat{F}_{\sigma}$ and solve for $\hat{G}_{\theta}$ given $\hat{F}_{\sigma}$. There are two compuatation methods.
\begin{itemize}
    \item Reexpress the Gaussian component as Student's $t$ therefore eliminating the
          dependence on $\sigma_i$.
    \item Iterate between the Gamma and Gaussian component of the likelihood. (Specific
          to this independent prior assumption.)
\end{itemize}
\subsection{Location-scale mixture (general)}
The most general Gaussian location-scale mixture with covariate effects
\begin{equation*}
    y_{it}=x_{it}\beta+\theta_i+\sigma_i\epsilon_{it} \quad \text{where} \quad \epsilon_{it}\sim N(0,1)
\end{equation*}
Given a true $\beta$, it is straightforward that \begin{equation*}
    y_{it}|\mu_i,\sigma_i,\beta \sim N(x_{it}\beta+\mu_i,\sigma_i^2)
\end{equation*}
The sufficient statistics for
\begin{itemize}
    \item $\mu_i$: $\bar{y}_i-\bar{x}_i\beta$
          \begin{quote}
              contains the between information
          \end{quote}
    \item $\sigma_i^2$: $\frac{1}{T_i-1}\sum_{t=1}^{T_i}(y_{it}-x_{it}\beta-\mu_i)^2$
          It is worth mentioning that \begin{equation*}
              S_i|\mu_i,\sigma_i^2,\beta \sim \gamma(r_i,\sigma_i^2/r_i) \quad \text{where} \quad r_i=(m_i-1)/2
          \end{equation*}
          \begin{quote}
              contains the within information (deviations from the individual means)
          \end{quote}
\end{itemize}
\begin{remark}
    The orthogonality between the within and between information no longer holds here. (Why does it hold in the classical Gaussian panel data?)
\end{remark}
The likelihood function is \begin{equation*}
    \begin{split}
        l(\beta, h(\theta,\sigma)|y)&=\prod_{i=1}^N g_i(\beta,\theta_i,\sigma_i|y_{i1},\ldots,y_{iT})\\
        &=\prod_{i=1}^N\int \int \prod \frac{1}{\sigma} \phi(\frac{y_{it}-x_{it}\beta-\theta_i}{\sigma}) h(\theta,\sigma)d\theta d\sigma\\
        & =K \prod_{i=1}^N\int S_i^{1-r_i}\int\int \frac{1}{\sigma} \phi(\frac{\bar{y}_{i}-\bar{x}_{i}\beta-\theta_i}{\sigma})\frac{e^{-R_i}R_i^{r_i}}{S_i\Gamma(r_i)}h(\theta, \sigma)d\theta d\sigma
    \end{split}
\end{equation*}
where \begin{equation*}
    \begin{array}{ll}
        R_i=\frac{r_iS_i}{\sigma_i^2} & K=\prod_{i=1}^N\ \pa{\frac{\Gamma(r_i)}{r_i^{r_i}}(\frac{1}{\sqrt{2\pi}})^{T_i-1}}
    \end{array}
\end{equation*}

\begin{question}
    The true $\beta$ is unknown. Therefore, we can not condition on it. How about the so called profile likelihood? How to compare it with the \textit{FORBIDDEN} approach of getting fixed effect estimates from the WG estimation? Since the $\theta_i$ is regarded as \textit{NUISANCE} parameters in the WG estimation, how low the status is...! Poor $\theta_i$!
\end{question}

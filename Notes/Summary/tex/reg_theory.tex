\newpage
\section{Regression tables}

\subsection{WG, BG, Random effect, Correalted random effect(Mundlak)}
\paragraph{Notation}
\begin{itemize}
    \item Dependent variable: \begin{equation*}
              \underbrace{y}_{NT\times 1}= \begin{pmatrix}
                  y_1    \\
                  \vdots \\
                  y_N
              \end{pmatrix} \quad
              \underbrace{y_i}_{T\times 1} = \begin{pmatrix}
                  y_{i1} \\
                  \vdots \\
                  y_{iT}
              \end{pmatrix}
          \end{equation*}
    \item Independent variable:\begin{equation*}
              \underbrace{X}_{NT\times K}= \begin{pmatrix}
                  x_1    \\
                  \vdots \\
                  x_N
              \end{pmatrix} \quad
              \underbrace{x_i}_{T\times K} = \begin{pmatrix}
                  x_{i1} \\
                  \vdots \\
                  x_{iT}
              \end{pmatrix}
          \end{equation*}
    \item Matrix to calcualte the mean: \begin{equation*}
              B_T= d_T(d_T' d_T)^{-1}d_T'\quad \text{where} \quad d_T=\begin{pmatrix}
                  1 \\ \vdots \\ 1 \end{pmatrix}
          \end{equation*}
          Thus \begin{equation*}
              B_T y_1 = \begin{pmatrix}
                  \bar{y}_1 \\ \vdots \\ \bar{y}_1
              \end{pmatrix}
          \end{equation*}
          we set $B=I_N\otimes B_T$
    \item Matrix to demean the variable: \begin{equation*}
              W_T= I_T-B_T
          \end{equation*}
          Thus, \begin{equation*}
              W_T y_i = \begin{pmatrix}
                  y_{i1}-\bar{y}_1 \\ \vdots \\ y_{iT}-\bar{y}_1
              \end{pmatrix}
          \end{equation*}
          we set $W=I_N\otimes W_T$

\end{itemize}
\paragraph{Estimators}
\begin{itemize}
    \item WG: \begin{equation*}
              \hat{\beta}_{WG}=(X'WX)^{-1}X'W y
          \end{equation*}
    \item BG: \begin{equation*}
              \hat{\beta}_{BG}=(X'B X)^{-1}X'By\end{equation*}
    \item RE: A linear combination of WG and BG
    \item CRE (Mundlak): equivalent to WG
          \begin{quote}
              In empirical analysis of data consisting of repeated observations on economic units (time series on a cross section) it is often assumed that the coefficients of the quantitiative variables (slopes) are the same, whereas the coefficients of the qualitative variables (intercepts or effects) vary over units or periods. This is the constant-slope variable- intercept framework. In such an analysis an explicit account should be taken of the statistical dependence that exists between the quantitative variables and the effects. It is shown that when this is done, the random effect approach and the fixed effect approach yield the same estimate for the slopes, the "within" estimate. Any matrix combination of the "within" and "between" estimates is generally biased. When the "within" estimate is subject to a relatively large error a minimum mean square error can be applied, as is generally done in regression analysis. Such an estimator is developed here from a somewhat different point of departure.
          \end{quote}
\end{itemize}
% Denote $y=\text{ETP_INF}$, $x_1=\text{STAC INPATIENT}$, $x_2=\text{STAC
%         OUTPATIENT}$, $x_3=\text{SESSION}$\
\subsection{Inference on $\hat{\beta}$ and $\hat{\theta_i}$}
\paragraph{Inference on $\beta$} Because
\begin{equation*}
    \hat{\beta}_{WG}-\beta = (X'WX)^{-1}X'W\epsilon
\end{equation*}
we have \begin{equation*}
    \sqrt{N}(\hat{\beta}_{WG}-\beta) \to^d N(0, A^{-1}C A^{-1})\\
\end{equation*}
where $A = \E\bra{X'WX}$ and $C = \E\bra{X'W\epsilon \epsilon'W'}=\E\bra{X'W\Omega W'X}$.\\
We may greatly simplify $\Omega$ if we assume that all individual and all observations' $\epsilon$ are iid. Then $\Omega = \sigma^2 I_{NT}$ (something that we generally do not assume.)\\

\paragraph{Inference on $\theta_i$}
If we apply the WG estimator $\hat{\beta}_{WG}= (X'WX)^{-1}X'W y$. The
estimated fixed effect is \begin{equation*}
    \begin{split}
        \hat{\theta}_i &= \frac{1}{T}\sum_{t=1}^T y_{it}-x_{it}\hat{\beta}_{WG}\\
        &= \frac{1}{T}\sum y_{it}-x_{it}\beta+\frac{1}{T}\sum x_{it}\beta-x_{it}\hat{\beta}_{WG}\\
        &= \theta_i+\frac{1}{T}\sum_t \epsilon_{it} + \pa{\frac{1}{T}\sum x_{it}}(\beta-\hat{\beta}_{WG})\\
    \end{split}
\end{equation*}
It is clear that the second part follows a normal distribution $N(0, \sigma_i^2)$. The third part is where asymptotic kicks since
\begin{equation*}
    \begin{split}
        \sqrt{N}\hat{\beta}_{WG} -\beta &\sim N(0, \Sigma)\\
        \hat{\beta}_{WG} -\beta &\sim N(0, \frac{1}{N}\Sigma)\\
        \frac{1}{T}\sum x_{it}(\hat{\beta}_{WG}-\beta)&\sim N(0, \pa{\frac{1}{T^2}\sum x_{it}^2}\frac{1}{N}\Sigma)\\
    \end{split}
\end{equation*}
Then $\hat{\theta}_i-\theta_i$ is a sum of two (non independent) normal variables. Each following $N(0, \frac{\sigma_i^2}{T})$ and $N(0, \pa{\frac{1}{T^2}\sum x_{it}^2}\frac{1}{N}\Sigma)$ respectively.
\begin{remark}
    We don't know the $\Sigma$ and there's a need to replace it by $\hat{\Sigma}$.
\end{remark}

\paragraph{Recall the lectures...}
Assume a simple stripped down model where $y_{it}=\alpha_i+\epsilon_{it}$, two
cases \begin{enumerate}
    \item $y_{it}=\alpha_i+\epsilon_{it} \sim N(\alpha_i,\sigma^2)$
    \item $y_{it}=\alpha_i+\epsilon_{it} \sim N(\alpha_i,\sigma_i^2)$
\end{enumerate}
The estimator of $\alpha_i$ is \begin{equation*}
    \hat{\alpha}_i = \bar{y}_i
\end{equation*}

\paragraph{Estimate the $\sigma^2$ and $\sigma_i^2$}
The classical incidental parameter problem appears when we want to estimate
$\sigma^2$ or $\sigma_i^2$ because we only have an estimate of $\hat{\alpha}_i
    = \bar{y}_i$.
\begin{enumerate}
    \item $\sigma^2$: \begin{equation*}
              \hat{\sigma}^2 = \frac{1}{NT}\sum_{i=1}^N\sum_{t=1}^T (y_{it}-\hat{\alpha}_i)^2
          \end{equation*}
          \begin{itemize}
              \item The mean of the estimator is $\E\bra{\hat{\sigma}^2} = \sigma^2
                        (1-\frac{1}{T})$. Asymptotically \textbf{BIASED}. (Though We can deploy bias
                    correction techniques.)
              \item The variance of the estimator is $\Var\pa{\hat{\sigma}^2} =
                        \frac{2\sigma^4}{NT}(1-\frac{1}{T})$
          \end{itemize}
    \item $\sigma_i^2$: \begin{equation*}
              \hat{\sigma}_i^2 = \frac{1}{T}\sum_{t=1}^T (y_{it}-\hat{\alpha}_i)^2
          \end{equation*} This corresponds to the \textit{sufficient statistics} we will be talking about later.
\end{enumerate}
\paragraph{Estimate the variance of $\alpha$} If we treat $\alpha_i$ for each $i$
as a fixed number, and be agnostic about the possible latent distribution
$G_{\alpha}$ from which $\alpha_i$ is drawn from. We don't talk about the
variance of $\alpha$. Yet if we instead believe that all $\alpha_i$ are drawn
iidly from a distribution $G$, then it is interesting to estimate something
about $G$ (e.g. the moment of $\alpha$). \\ For example, we want to estimate
the \textbf{first moment} of $\alpha$ by \begin{equation*}
    \begin{split}
        \frac{1}{N}\sum \hat{\alpha}_i &= \frac{1}{N}\sum \alpha_i + \frac{1}{N}\sum \bar{\epsilon}_i\\
        \E\bra{\frac{1}{N}\sum \hat{\alpha}_i} &= \E\bra{\alpha_i} + 0
    \end{split}
\end{equation*}
This is unbiased.\\
Yet if we want to estimate the \textbf{second moment} of $\alpha$ by \begin{equation*}
    \begin{split}
        \frac{1}{N}\sum (\hat{\alpha}_i)^2 &= \frac{1}{N}\sum (\alpha_i+\bar{\epsilon}_i)^2\\
        &= \frac{1}{N}\sum \alpha_i^2 + \frac{1}{N}\sum \bar{\epsilon}_i^2 + \frac{2}{N}\sum \alpha_i\bar{\epsilon}_i\\
        \E\bra{\frac{1}{N}\sum (\hat{\alpha}_i)^2} &= \E\bra{\alpha_i^2} + \frac{1}{T}\sigma_i^2+0\\
    \end{split}
\end{equation*}
This is biased and the bias is $\frac{1}{T}\sigma_i^2$.

This is the issue we encountered when we go from the first moment of $\alpha$
to the second moment. It would be interesting to estimate the
$G_{\alpha}(\alpha_i)$ directly using non parametric convex optimization
methods.\\ Or when $\sigma_i^2$ is heterogeneous, it would be
(challenging/interesting) to estimate $H_{\alpha,\sigma}(\alpha_i,\sigma_i)$
directly.

\subsection{Multiplcative fixed effect with two types of errors}
The multiplicative fixed effect model is \begin{equation*}
    \begin{split}
        y_{it} = f(x_{it};\beta)\theta_i
    \end{split}
\end{equation*}
\paragraph{Multiplicative error}
\begin{equation*}
    y_{it} = f(x_{it};\beta)\theta_i\epsilon_{it}
\end{equation*}
The exogeneity assumption is that $\E\bra{\epsilon_{it}|x_{it},\theta_i}=1$.\\
The moment condition is that \begin{equation*}
    \E\bra{x_{it}\pa{\frac{y_{it}}{f(x_{it};\beta)\theta_i}-1}}=\E\bra{x_{it} \E\bra{\frac{y_{it}}{f(x_{it};\beta)\theta_i}-1|x_{it},\theta_i}}=0
\end{equation*}
Given $\theta_i$ known, the emipircal moment condition to estimate $\beta$ is \begin{equation}\label{eq:mult_error_beta}
    \sum_{i=1}^N\sum_{t=1}^T x_{it}\pa{\frac{y_{it}}{f(x_{it};{\beta}){\theta}_i}-1}=0
\end{equation}
Given $\beta$ known, the emipircal moment condition to estimate $\theta_i$ is \begin{equation}
    \label{eq:mult_error_theta}
    \begin{split}
        &\sum_{t=1}^T {\frac{y_{it}}{f(x_{it};{\beta})\theta_i}-1}=0\\
        & \Leftrightarrow \hat{\theta}(\beta)=\frac{1}{T}\sum_{t=1}^T \frac{y_{it}}{f(x_{it};\beta)}\\

    \end{split}
\end{equation}
Since $\theta_i$ is unknown, We can use \ref{eq:mult_error_beta} to get the profiling estimating equation for $\beta$ which is \begin{equation*}
    \sum_{n=1}^N\sum_{t=1}^T x_{it}\pa{\frac{y_{it}}{f(x_{it};\beta)\hat{\theta}_i(\beta)}-1}=0
\end{equation*}
The profiling estimating equation is biased, which can be seen from (taking expectation conditioning on all $x_{it}$ and evaluate at the true $\beta$) \begin{equation*}
    \begin{split}
        \sum_{n=1}^N\sum_{t=1}^T \E\bra{x_{it}\E\pa{\frac{\theta_i\epsilon_{it}}{\hat{\theta}_i}-1\bigg |x_{it}}}\\&=\sum \sum \E\bra{x_{it}\E\bra{\frac{\theta_i\epsilon_{it}}{\frac{1}{T}\sum{\theta_i \epsilon_{it}}}-1\bigg | x_{it}}}\\& = \sum \sum \E\bra{x_{it}\E \bra{\frac{\epsilon_{it}}{\bar{\epsilon}_{i}}-1\bigg | x_{it}}}=0
    \end{split}
\end{equation*}
This is unbiased only when $\E\bra{\frac{\epsilon_{it}}{\bar{\epsilon}_i}|x_{it}}=1$. One of the situations that this is true is $\epsilon_{it}|x_{it}$ is iid. Because $\E\bra{\frac{\epsilon_{it}}{\bar{\epsilon}_i}}=\frac{1}{T}\sum_{t=1}^T \E\bra{\frac{\epsilon_{it}}{\bar{\epsilon}_i}}=\E\bra{\frac{\bar{\epsilon}_i}{\bar{\epsilon}_i}}=1$

\paragraph{Additive error}
\begin{equation*}
    y_{it} = f(x_{it};\beta)\theta_i+u_{it}
\end{equation*}
The exogeneity assumption is that $\E\bra{u_{it}|x_{it},\theta_i}=0$.\\
The equivalence can be shown since $u_{it}=f(x_{it};\beta)\theta_i(1-\epsilon_{it})$. Thus, \begin{equation*}
    \E\bra{u_{it}|x_{it},\theta_i}=f(x_{it};\beta)\theta_i\E\bra{(1-\epsilon_{it})|x_{it},\theta_i} \Leftrightarrow \E\bra{\epsilon_{it}|x_{it},\theta_i}=1
\end{equation*}
The moment condition is that \begin{equation*}
    \E\bra{x_{it}\pa{y_{it}-f(x_{it};\beta)\theta_i}}=\E\bra{x_{it} \E\bra{y_{it}-f(x_{it};\beta)\theta_i|x_{it},\theta_i}}=0
\end{equation*}
Given $\theta_i$, the emipircal moment condition to estimate $\beta$ is \begin{equation}
    \label{eq:add_error_beta}
    \sum_{i=1}^N\sum_{t=1}^T x_{it}\pa{y_{it}-f(x_{it};{\beta}){\theta}_i}=0
\end{equation}
Given $\beta$, the emipircal moment condition to estimate $\theta_i$ is \begin{equation}
    \label{eq:add_error_theta}
    \sum_{t=1}^T {y_{it}-f(x_{it};{\beta})}=0
\end{equation}
Since $\theta_i$ is unknown, We can use \ref{eq:add_error_beta} to get the profiling estimating equation for $\beta$ which is \begin{equation*}
    \sum_{n=1}^N\sum_{t=1}^T{y_{it}-f(x_{it};\beta)\hat{\theta}_i(\beta)}=0
\end{equation*}
where $\hat{\theta}_i(\beta)=\frac{\sum_t y_{it}}{\sum_tf(x_{it};\beta)}=\frac{\sum f*\theta_i+u_{it}}{\sum f}=\theta_i+\frac{\sum_t u_{it}}{\sum_t f}$
The unbiasedness condtion requires that \begin{equation*}
    \E\bra{y_{it}-f(x_{it};\beta)\frac{\sum_t y_{it}}{\sum_tf(x_{it};\beta)}|x_{it}}=0
\end{equation*}
which boils down to \begin{equation*}
    \E\bra{\hat{\theta}_i|x_{it}}=\theta_i
\end{equation*} which is true.
\begin{remark}
    If we use intrument $z_{it}$ the unbiasedness condition would be \begin{equation*}
        \E\bra{y_{it}-\frac{\sum_t y_{it}}{\sum_tf(x_{it};\beta)}\bigg |z_{it},x_{it}}=0
    \end{equation*} which is generally not true.
\end{remark}

\paragraph{Poisson distribution}
We assume that $y_{it}|x_{i1},\ldots,x_{iT},\theta_i \sim
    \calp(f(x_{it};\beta)\theta_i)$. Then $\beta$ are estimated jointly by MLE.
% 有点混乱之我到底应该怎么assume error term. Multiplicative or additive error? The additive form is the standard Pseudo Poisson estimator?
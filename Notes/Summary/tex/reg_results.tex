\newpage
\section{Regression tables}

\subsection{WG, BG, Random effect, Correalted random effect(Mundlak)}
\paragraph{Notation}
\begin{itemize}
    \item Dependent variable: \begin{equation*}
              \underbrace{y}_{NT\times 1}= \begin{pmatrix}
                  y_1    \\
                  \vdots \\
                  y_N
              \end{pmatrix} \quad
              \underbrace{y_i}_{T\times 1} = \begin{pmatrix}
                  y_{i1} \\
                  \vdots \\
                  y_{iT}
              \end{pmatrix}
          \end{equation*}
    \item Independent variable:\begin{equation*}
              \underbrace{X}_{NT\times K}= \begin{pmatrix}
                  x_1    \\
                  \vdots \\
                  x_N
              \end{pmatrix} \quad
              \underbrace{x_i}_{T\times K} = \begin{pmatrix}
                  x_{i1} \\
                  \vdots \\
                  x_{iT}
              \end{pmatrix}
          \end{equation*}
    \item Matrix to calcualte the mean: \begin{equation*}
              B_T= d_T(d_T' d_T)^{-1}d_T'\quad \text{where} \quad d_T=\begin{pmatrix}
                  1 \\ \vdots \\ 1 \end{pmatrix}
          \end{equation*}
          Thus \begin{equation*}
              B_T y_1 = \begin{pmatrix}
                  \bar{y}_1 \\ \vdots \\ \bar{y}_1
              \end{pmatrix}
          \end{equation*}
          we set $B=I_N\otimes B_T$
    \item Matrix to demean the variable: \begin{equation*}
              W_T= I_T-B_T
          \end{equation*}
          Thus, \begin{equation*}
              W_T y_i = \begin{pmatrix}
                  y_{i1}-\bar{y}_1 \\ \vdots \\ y_{iT}-\bar{y}_1
              \end{pmatrix}
          \end{equation*}
          we set $W=I_N\otimes W_T$

\end{itemize}
\paragraph{Estimators}
\begin{itemize}
    \item WG: \begin{equation*}
              \hat{\beta}_{WG}=(X'WX)^{-1}X'W y
          \end{equation*}
    \item BG: \begin{equation*}
              \hat{\beta}_{BG}=(X'B X)^{-1}X'By\end{equation*}
    \item RE: A linear combination of WG and BG
    \item CRE (Mundlak): equivalent to WG
          \begin{quote}
              In empirical analysis of data consisting of repeated observations on economic units (time series on a cross section) it is often assumed that the coefficients of the quantitiative variables (slopes) are the same, whereas the coefficients of the qualitative variables (intercepts or effects) vary over units or periods. This is the constant-slope variable- intercept framework. In such an analysis an explicit account should be taken of the statistical dependence that exists between the quantitative variables and the effects. It is shown that when this is done, the random effect approach and the fixed effect approach yield the same estimate for the slopes, the "within" estimate. Any matrix combination of the "within" and "between" estimates is generally biased. When the "within" estimate is subject to a relatively large error a minimum mean square error can be applied, as is generally done in regression analysis. Such an estimator is developed here from a somewhat different point of departure.
          \end{quote}
\end{itemize}
% Denote $y=\text{ETP_INF}$, $x_1=\text{STAC INPATIENT}$, $x_2=\text{STAC
%         OUTPATIENT}$, $x_3=\text{SESSION}$\
\paragraph{Inference on $\theta_i$ and ($\beta$)}
If we apply the WG estimator $\hat{\beta}_{WG}= (X'WX)^{-1}X'W y$. The
estimated fixed effect is \begin{equation*}
    \begin{split}
        \hat{\theta}_i &= \frac{1}{T}\sum_{t=1}^T y_{it}-x_{it}\hat{\beta}_{WG}\\
        &= \frac{1}{T}\sum y_{it}-x_{it}\beta+\frac{1}{T}\sum x_{it}\beta-x_{it}\hat{\beta}_{WG}\\
        &= \theta_i+\frac{1}{T}\sum_t \epsilon_{it} + \frac{1}{T}\sum x_{it}(\beta-\hat{\beta}_{WG})\\
        & = \theta_i + \frac{1}{T}\sum_t \epsilon_{it} + \frac{1}{T}(X'WX)^{-1}X'W\epsilon \sum_t x_{it}\\
    \end{split}

\end{equation*}
\begin{question}
    How to estimate the variance of $\hat{\theta}_i$? How to estimate the variance of $\alpah$?
\end{question}

\subsection{Specification}\hypertarget{home}{}
\paragraph{Model 1} (Pseudoc Poisson)
\begin{equation*}
    \begin{split}
        y_{it}&=x_{it1}^{\beta_1}x_{it2}^{\beta_2}x_{it3}^{\beta_3}\theta_i\epsilon_{it}\\
        &= f(x_{it};\beta)\theta_i\epsilon_{it}\\
    \end{split}
\end{equation*}

Tables: \hyperlink{reg_inf_pois_2022}{Pois}

\paragraph{Reference} to Koen Jochman's lecture notes on panel data and model with multiplicative
effect:
\begin{equation*}
    y_{it}=f(x_{it};\beta)\theta_i\epsilon_{it} \quad \text{where} \quad \E\bra{\epsilon_{it}|x_{i1},\ldots,x_{iT},\theta_i}=1
\end{equation*}
Then following the same logic in additive effect, we difference out individual effect to get:
\begin{equation*}
    \E\bra{\frac{y_{it}}{f(x_{it};\beta)}-\frac{y_{i,t-1}}{f(x_{i,t-1};\beta)}\bigg | x_{i1},\ldots,x_{iT}}=0
\end{equation*}
Similarly, \begin{equation*}
    \E\bra{\frac{y_{it}}{f(x_{it};\beta)}-\frac{\sum_{t=1}y_{it}}{\sum_{t=1}f(x_{it};\beta)}\bigg | x_{i1},\ldots,x_{iT}}=0
\end{equation*}
One of the unconditional moment equation given rise to is \begin{equation}
    \E\bra{x_it\pa{\frac{y_{it}}{f(x_{it};\beta)}-\frac{\sum_{t=1}y_{it}}{\sum_{t=1}f(x_{it};\beta)}}}=0
\end{equation}
The moment conditon is often called pseudo-poisson estimator (as if assuming $y_it|x_{i1},\ldots,x_{iT},\theta_i\sim Poisson(f(x_{it};\beta)\theta_i)$ and then use maximum likelihood.)
When regressors are not strictly exogenous, we can construct a \textbf{differencing} based estimator based on sequential moment restrictions.
The \verb+fixest+ package doesn't provide the differencing based estimator. \\
\textbf{TO BE CONSTRUCTED BY HAND LATER.}
\paragraph{Model 2} (OLS)
\begin{align*}
    \log(y_{it}) & =\log(x_{it1})\beta_1+\log(x_{it2})\beta_2+\log(x_{it3})\beta_3+\log(\theta_i)+\log(\epsilon_{it}) \\
\end{align*}
LHS: log(ETP INF); \\ RHS: log(STAC INPATIENT), log(STAC OUTPATIENT),
log(SESSION),CASEMIX
\\Table: \hyperlink{reg_inf_ols_2022}{OLS},
\hyperlink{reg_inf_lag_2022}{OLS\_lag1}
\\Figure: \hyperlink{FE_OLS_FI}{FixedEffect\_OLS}

\begin{remark}
    The Pseudo poisson and Log OLS are equivalent if we assume that $\epsilon_{it}$ is independent of $x_{it}$ and $\theta_i$.
\end{remark}

\begin{remark}
    The Mundlak (1978) approach is to include the average of the individual-specific variables $\bar{x_i}$ in the regression. He shows that it is equivalent to within group estimator.
    (Correlated random effect $\sim$ WG estimator). If the true model is \begin{equation*}
        y_{it}=x_{it}\beta+\theta_i+\epsilon_{it}
    \end{equation*} and $E(\theta_i|\bar{x}_i)=\bar{x}_i\gamma+\tilde{\theta}_i$, then
    \begin{equation*}
        y_{it}=(x_{it}-\bar{x}_i)\beta+\bar{x}_i(\gamma+\beta)+\tilde{\theta}_i+\epsilon_{it}
        (x_{it}-\bar{x}_i)\beta_1+\bar{x}_i\beta_2+\tilde{\theta}_i+\epsilon_{it}
    \end{equation*}
    Only when the $\theta_i$ is uncorrelated with $x_{it}$, the $\beta_1 = \beta_2$.
\end{remark}

\newpage
\hypertarget{reg_inf_pois_2022}{Pois}
\input{../../Tables/2016-2022/reg_inf_pois.tex}
\hyperlink{home}{Back}
\bigskip

\newpage
\hypertarget{reg_inf_ols_2022}{OLS}
\input{../../Tables/2016-2022/reg_inf_ols.tex}
\hyperlink{home}{Back}
\bigskip
\newpage
\hypertarget{reg_inf_lag_2022}{OLS\_lag1}
\input{../../Tables/2016-2022/reg_inf_lag.tex}
\hyperlink{home}{Back}
\bigskip
\newpage
\hypertarget{FE_OLS_FI}{FixedEffect\_OLS} Fixed effects extracted from OLS regression with fixed effect on each FI (not FI\_EJ).
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../../Figures/2016-2022/FE_ols_FI.pdf}
    \includegraphics[width=0.8\textwidth]{../../Figures/2016-2022/FE_ols_FI_e.pdf}
\end{figure}
\hyperlink{home}{Back}
\bigskip
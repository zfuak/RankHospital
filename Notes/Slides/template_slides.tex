\documentclass[10pt,mathserif,aspectratio=169]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag,booktabs,natbib}

\input {newcommand.tex}

%% formatting

\mode<presentation>
{
  \usetheme{default}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
  \begin{center}
    {\large\bf \insertframetitle}
  \end{center}
}

\newcommand\footlineon{
  \setbeamertemplate{footline} {
    \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,leftskip=.8cm,rightskip=.6cm]{structure}
      \footnotesize \insertsection
      \hfill
      {\insertframenumber}
    \end{beamercolorbox}
    \vskip 0.45cm
  }
}
\footlineon

% \AtBeginSection[]
% {
%   \begin{frame}<beamer>
%     \frametitle{Outline}
%     \tableofcontents[currentsection,currentsubsection]
%   \end{frame}
% }

%% begin presentation

\title{\large \bfseries L'HÃ´pital's (Selection) Rule\\
  An Empirical Bayes Application to French Hospitals}

\author{Fu Zixuan\\[3ex]
  Supervised by Prof.Thierry Magnac}
% }

\date{\today}

\begin{document}

\frame{
  \thispagestyle{empty}
  \titlepage
}

\section{Introduction}

\begin{frame}
  \frametitle{Questions}
  \begin{itemize}\itemsep=12pt
    \item Out of the top 20\% hospitals in France\footnote{in terms of labor (nurses)
            employment efficiency}, how many of them are public hospitals/private
          hospitals?
    \item What would be the selection outcome if I want to control the number of mistakes
          that I make?
    \item Does different selection rule produce different results? And to what degree?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Roadmap}
  \begin{enumerate}\itemsep=12pt
    \item Estimate the efficiency.
          \begin{itemize} \itemsep=8pt
            \item $Y$: Labor input (number of full time equivalent nurses).
            \item $X$: Hospital output (e.g., inpatient/outpatient stays, medical sessions).
          \end{itemize}
    \item \textbf{Select the 20\% most efficient hospitals.}

  \end{enumerate}
\end{frame}

\begin{frame}[label=literature]
  \frametitle{Literature: Measuring efficiency of individual units}
  \begin{itemize}\itemsep=12pt

    \item \textit{Productivity/Efficiency}: Factories, Schools, Hospitals etc.
    \item \textit{Ownership}: Public (Teaching, Ordinary) vs. Private (For profit, Non-profit).
    \item \textit{Methodology}: Following \citep{croiset2024hospitals}, we use the \textit{conditional input demand function} \hyperlink[inputdemand]{\beamergotobutton{Reasons}}
          \begin{equation*}
            \log(y_{it,\text{nurses}}) = x_{it,\text{output}}\beta +\theta_i +\varepsilon_{it} \quad \text{where}\quad \varepsilon_{it} \sim \caln(0,\sigma_i^2)
          \end{equation*}
          The smaller the $\theta_i$, the less input is needed to produce the same amount of output, the more efficient the hospital is.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Literature: Invidious decision}
  \begin{itemize}\itemsep=12pt
    \item \textit{League table mentality}: Ranking \& Selection.\citep{gu2023invidious}
    \item \textit{Noisy estimates}: Unobserved heterogeneity. \citep{chetty2014measuring,kline2022systemic}
    \item \textit{Compound Decision/ Empirical Bayesian}: Compound decision framework \citep{robbins1956empirical}, (Non-parametric) Estimation of a prior distribution. \citep{koenker2014convex, gu2017empirical}
  \end{itemize}
\end{frame}

\section{Data and Estimation}

\begin{frame}
  \frametitle{Hospital Types}
  \begin{table}
    \fontsize{10pt}{10pt}\selectfont
    \begin{tabular}{rrrrrr}
      \toprule
      Year & Teaching & Normal Public & Private FP & Private NP & Total \\
      \midrule
      2013 & 198      & 1312          & 1305       & 1382       & 4197  \\
      2014 & 201      & 1274          & 1293       & 1349       & 4117  \\
      2015 & 211      & 1275          & 1297       & 1349       & 4132  \\
      2016 & 212      & 1266          & 1297       & 1313       & 4088  \\
      2017 & 211      & 1249          & 1297       & 1306       & 4063  \\
      2018 & 214      & 1247          & 1296       & 1288       & 4045  \\
      2019 & 214      & 1236          & 1287       & 1281       & 4018  \\
      2021 & 219      & 1222          & 1293       & 1264       & 3998  \\
      2022 & 220      & 1220          & 1296       & 1259       & 3995  \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}
  \frametitle{Output}
  \begin{table} \fontsize{8pt}{10pt}\selectfont
    \begin{tabular}{lllll}
      \toprule
      Output                                         & Normal Public & Private Non Profit & Private For Profit & Teaching \\
      \midrule
      STAC\footnote{Short term acute care} inpatient & 8.08\%        & 5.66\%             & 16.3\%             & 7.9\%    \\
      STAC oupatient                                 & 2.26\%        & 4.02\%             & 22.61\%            & 3.59\%   \\
      Sessions                                       & 4.34\%        & 23.31\%            & 27.17\%            & 4.8\%    \\
      Outpatient Consultations                       & 58.23\%       & 43.55\%            & 0.8\%              & 69.18\%  \\
      Emergency                                      & 21.14\%       & 6.78\%             & 17.3\%             & 12.64\%  \\
      Follow-up care and Long-term care              & 1.67\%        & 11.26\%            & 12.16\%            & 1.09\%   \\
      Home hospitalization                           & 0.06\%        & 0.76\%             & 0.17\%             & 0.08\%   \\
      Psychiatry stays                               & 4.22\%        & 4.66\%             & 3.49\%             & 0.72\%   \\
      \bottomrule
    \end{tabular}
  \end{table}
  The hospitals differ not only in efficiency but also in the mix of services they provide.
\end{frame}
\begin{frame}{Fixed effect estimation}
  System GMM: use lagged difference as instruments for current levels
  \begin{equation*}
    \E[\Delta x_{i,t-1}(y_{it}-\beta x_{it})] \quad \text{if}\quad \E\bra{\Delta x_{i,t-1}(\theta_i+\varepsilon_{i,t})}=0
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Results}
  \begin{table}\fontsize{6pt}{8pt}\selectfont
    \begingroup
    \centering
    \begin{tabular}{lccc}
      \tabularnewline \midrule \midrule
      Dependent Variable: & \multicolumn{3}{c}{log(ETP\_INF)}                                          \\
                          & Within Group                      & First Difference & \textit{System GMM} \\
      Model:              & (1)                               & (2)              & (3)                 \\
      \midrule
      \emph{Variables}                                                                                 \\
      log(SEJHC\_MCO)     & $0.10^{***}$                      & $0.07^{***}$     & $0.70^{***}$        \\
                          & $(0.00)$                          & $(0.01)$         & $(0.05)$            \\
      log(SEJHP\_MCO)     & $0.02^{***}$                      & $0.01^{***}$     & $-0.05$             \\
                          & $(0.00)$                          & $(0.00)$         & $(0.04)$            \\
      $\cdots$            & $\cdots$                          & $\cdots$         & $\cdots$            \\
      log(SEANCES\_MED)   & $0.02^{***}$                      & $0.02^{***}$     & $0.07^{***}$        \\
                          & $(0.00)$                          & $(0.00)$         & $(0.03)$            \\

      log(SEJ\_PSY)       & $0.00$                            & $0.00$           & $0.07^{***}$        \\  & $(0.00)$ & $(0.00)$ &
      $(0.01)$                                                                                         \\ \midrule \emph{Fit statistics} \\ Observations & $15335$                     & $13502$
                          & $11536$                                                                    \\

      n                   & 1833                              & 1833             & $1833$              \\ T & 9 & 9 & $9$ \\ \midrule \midrule
      \multicolumn{4}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}                            \\
    \end{tabular}
    \par\endgroup
  \end{table}
\end{frame}

\section{Compound decision and Empirical Bayes}

\begin{frame}
  \frametitle{Compound Decision Framework}
  Observe:
  \begin{align*}
    \boldsymbol{\hat{\theta}} & =  (\hat{\theta}_1,\ldots, \hat{\theta}_n)  \\
    \text{where} \quad        & \hat{\theta}_i | \theta_i \sim P_{\theta_i}
  \end{align*}
  Decision:
  \begin{equation*}
    \delta(\boldsymbol{\hat{\theta}}) = (\delta_1(\boldsymbol{\hat{\theta}}), \ldots, \delta_n(\boldsymbol{\hat{\theta}}))
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Compound Loss and Risk}
  Loss:
  \begin{equation*}
    L_n(\theta, \delta(\boldsymbol{\hat{\theta}})) = \sum_{i=1}^n L(\theta_i, \delta_i(\hat{\theta})).
  \end{equation*}
  Risk (Expectation of loss):
  \begin{align*}
    R_n(\theta, \delta(\boldsymbol{\hat{\theta}})) & = \E[L_n(\theta, \delta(\boldsymbol{\hat{\theta}}))]                                                                                 \\
                                                   & = \frac{1}{n}\sum_{i=1}^n \E[L(\theta_i, \delta_i(\boldsymbol{\hat{\theta}}))]         \quad \text{Separable decision rule} $\delta$ \\
                                                   & = \frac{1}{n}\sum_{i=1}^n \int L(\theta_i, \delta_i(\hat{\theta}_i))dP_{\theta_i}(\hat{\theta}_i)                                    \\
                                                   & = \int \int L(\theta_i, \delta(\hat{\theta}_i))dP_{\theta_i}(\hat{\theta}_i)dG_n(\theta)
  \end{align*}
  where $G_n(\alpha)$ is the empirical distribution\footnote{$E_{G_n}(f(x)) = 1/n \sum_i f(x_i)$} of $\theta_i$. (Frequentist View)
  $\longrightarrow$ Bayesian view: replace $G_n$ by a distribution $G$\footnote{$\theta_i \sim G$}. $\longrightarrow$ Empirical Bayes: Estimate the $G$.
\end{frame}

\begin{frame}
  \frametitle{The Selection Task}
  \begin{itemize}\itemsep=12pt
    \item Select the bottom 20\% \footnote{The most efficient 20\%.} of the true
          $\theta_i$. Since we assume that $\theta_i \sim G$, those $i$ whose
          $\theta_i<G^{-1}(0.2)$
    \item Control the overall false discovery rate at 20\%, \begin{equation*}
            \frac{\E_G\bra{1\set{\theta_i>\theta_{\alpha},\delta_i=1}}}{\E_G\bra{\delta_i}} \le \gamma
          \end{equation*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Problem Formulation}
  The \textbf{loss} function \footnote{$h_i$ is an indicator of whether the true value belong to the set. $\delta_i$ is an indicator of whether $i$ is being selected.} is
  \begin{equation*}
    L(\delta,\theta)=\sum h_i(1-\delta_i) +\tau_1\pa{\sum (1-h_i)\delta_i -\gamma \delta_i} + \tau_2 \pa{\sum \delta_i -\alpha n}
  \end{equation*} where $h_i=1\set{\theta_i<\theta_{\alpha}=G^-1(\alpha)}$.
  Therefore, the problem is to find $\delta$ such that
  \begin{align*}
    \min_{\delta} \quad & \E_G\E_{\theta|\hat{\theta}}\bra{L(\delta,\theta)}                                                                                                       \\
    =                   & \E_G \ \sum \E(h_i)(1-\delta_i) +\tau_1\pa{\sum (1-\E(h_i))\delta_i -\gamma \delta_i}                                                                    \\
                        & + \tau_2 \pa{\sum \delta_i -\alpha n}                                                                                                                    \\
    =                   & \E_G{\sum v_\alpha(\hat{\theta})(1-\delta_i) +\tau_1\pa{\sum (1-v_\alpha(\hat{\theta}))\delta_i -\gamma \delta_i} + \tau_2 \pa{\sum \delta_i -\alpha n}}
  \end{align*} where $v_\alpha(\hat{\theta})=\p(\theta<\theta_\alpha|\hat{\theta})$ is the \textbf{posterior tail probability} $\to$ Need the prior $G$ to derive $v$!
\end{frame}

\begin{frame}{Normality assumption on $\varepsilon_{it}$}
  Estimate the fixed effect $\theta_i$ by
  \begin{align*}
    \hat{\theta}_i =                       & \frac{1}{T}\sum(\theta_i+\varepsilon_{it}+x_{it}(\beta-\hat{\beta})) \\
    \overset{N\to \infty}{\longrightarrow} & \theta_i+\frac{1}{T}\sum_t \varepsilon_{it}
  \end{align*}
  When $T$ is relatively small (or even fixed), can't use central limit theorem to claim that $\hat{\theta}_i \overset{d}{\to} \caln(\theta_i,\frac{\sigma_i^2}{T})$.
  $\longrightarrow$ Assume that $\varepsilon_{it} \sim \caln(0,\sigma_i^2)$ .
  \hyperlink{observation}{\beamergotobutton{Back (main)}}   \hyperlink{limitation}{\beamergotobutton{Back (end)}}
\end{frame}

\begin{frame}[label=observation]
  \frametitle{Prior Distribution $G$}
  Observe\footnote{$Y_{it}=\theta_i+\varepsilon_{it}+x_{it}(\beta-\hat{\beta})$}
  \begin{equation*}
    Y_{it} = \theta_i + \varepsilon_{it} \quad \varepsilon_{it} \sim \caln(0,\sigma_i^2) \quad (\theta_i,\sigma_i^2) \sim G
  \end{equation*}
  Neither $\theta_i$ nor $\sigma_i^2$ is known. But the sufficient statistics are
  \begin{align*}
    Y_i=\frac{1}{T_i}\sum_{t=1}^{T_i}Y_{it}           & \quad \text{where}\quad Y_i|\theta_i,\sigma_i^2 \sim \caln(\theta_i,\sigma_i^2/T_i)     \\
    S_i=\frac{1}{T_i-1}\sum_{t=1}^{T_i}(Y_{it}-Y_i)^2 & \quad \text{where} \quad S_i|\sigma_i^2 \sim \Gamma(r_i= (T_i-1)/2,2\sigma_i^2/(T_i-1))
  \end{align*}
  \hyperlink{normality}{\beamergotobutton{Appendix}}
\end{frame}

\begin{frame}
  \frametitle{Tail probability}
  Given the two sufficient statistics, the posterior tail probability is
  \begin{align*}
    v_\alpha(Y_i,S_i) & = P( \theta_i < \theta_{\alpha} | Y_i,S_i)                                                                                 \\
                      & = \frac{{\int_{-\infty}^{\theta_{\alpha}} \Gamma(s_i|r_i,\sigma_i^2) f(y_i|\theta_i, \sigma_i^2) dG(\theta_i,\sigma_i^2)}}
    {{\int_{-\infty}^{\infty} \Gamma(s_i|r_i,\sigma_i^2) f(y_i|\theta_i, \sigma_i^2) dG(\theta_i,\sigma_i^2)}}
  \end{align*}
  We want to find a cutoff $\lambda$ such that both constraints are satisfied \footnote{Relaxed discrete optimization problem, following \citep{basu2018weighted}}:\\
  \begin{itemize}\itemsep=8pt
    \item Capacity: $\int \int P(v_\alpha(Y_i, S_i) > \lambda) dG(\theta_i,\sigma_i^2)
            \leq \alpha$
    \item FDR: $\int \int
            \frac{E[1\{v_\alpha(Y_i,S_i)>\lambda\}(1-v_\alpha(Y_i,S_i))]}{E[1\{v_\alpha(Y_i,S_i)>\lambda\}]}
            dG(\theta_i,\sigma_i^2) \leq \gamma$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Estimate $G$}
  Following \citep{koenker2014convex,andersen2010mosek}
  The primal problem:
  \begin{equation*}
    \min_{f=dG}\set{-\sum_i \log g(y_i)\bigg |g_i = T(f_i),\ K(f_i)=1,\ \forall i }
  \end{equation*}
  where $ T(f_i)=\int p(y |\alpha)f_id\alpha $ and  $K(f_i)= \int f_i d\alpha$.\\
  Discretize the support:
  \begin{equation*}
    \min_{f=dG}\left\{-\sum_i \log g(y_i)\bigg |g=Af,\ {1^T}f=1\right\}
  \end{equation*}
  where $A_{ij}= p(y_i|\alpha_j) $ and $ f = (f(\alpha_1),f(\alpha_2),\ldots,f(\alpha_m))$.\\
  The dual problem:
  \begin{equation*}
    \max_{\lambda,\mu} \left\{ \sum_i \log \lambda_1(i) \bigg| A^T\lambda_1 < \lambda_2 1,\ (\lambda_1>0) \right\}
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{ The estimated $\hat{G}$}
  \begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
    Case 1: $\sigma_i$ unknown, only $S_i$ observed.
    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{../../Figures/2013-2022/GMM_m/GLVmix_s.pdf}
    \end{figure}

    \column{0.5\textwidth}
    Case 2: $\sigma_i$ known.
    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix_s.pdf}
    \end{figure}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Unknown $\sigma_i$: Posterior Tail probability}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLVmix/Left_0.2_0.2_TPKWs.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Unknown $\sigma_i$: Posterior Mean}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLVmix/Left_0.2_0.2_PMKWs.pdf}
  \end{figure}
\end{frame}

% \begin{frame}
%   \frametitle{Unknown$\sigma_i$: "Face value"}
%   \begin{figure}
%     \centering
%     \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLVmix/Left_0.2_0.2_MLE.pdf}
%   \end{figure}
% \end{frame}

\begin{frame}[label=tpselect]
  \frametitle{Known $\sigma_i$: Posterior Tail probability}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Left_0.2_0.2_TPKWs.pdf}
  \end{figure}
  \hyperlink{tpcontour}{\beamergotobutton{Next}}
\end{frame}

\begin{frame}
  \frametitle{Known $\sigma_i$: Posterior Mean}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Left_0.2_0.2_PMKWs.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{"Face value"}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Left_0.2_0.2_MLE.pdf}
  \end{figure}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
  \begin{itemize}
    \item Difference in whether to assume known $\sigma_i$.
    \item Control for the False Discovery Rate.
    \item Private (FP and NP) hospitals are indeed more "efficient".
  \end{itemize}
\end{frame}

\begin{frame}[label=limitation]{Limitation}
  \begin{itemize}\itemsep=12pt
    \item Interpretation of the $\theta_i$.
    \item Specification, endogeneity \textit{etc.}
    \item Normality assumption on $\varepsilon_{it}$.
          \hyperlink{normality}{\beamergotobutton{Next}}
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
  \frametitle{References}
  \bibliography{../Thesis/ref.bib}
  \bibliographystyle{apalike}
\end{frame}

\appendix

\begin{frame}[label=inputdemand]{Conditional Input Demand Function}
  In standard microeconomics, the profit maximization problem is
  \begin{equation*}
    \max_{\vec{y}} \sum k_i y_i - \sum p_i x_i \quad \text{subject to} \quad f_i(x_1, x_2, \ldots, x_n) = y_i
  \end{equation*}
  where $p_i$ is the price of input $i$ and $f$ is the cost function.

  The cost minimization problem is thus
  \begin{equation*}
    \min_{\vec{x}} \sum p_i x_i \quad \text{subject to} \quad f_i(x_1, x_2, \ldots, x_n) = y_i
  \end{equation*}
  Thus, the factor demand function/correspondence is
  \begin{equation*}
    x_i = x_i(p_1, p_2, \ldots, p_n, y_1, y_2, \ldots, y_m)
  \end{equation*}

\end{frame}

\begin{frame}[label=production]{Input demand function vs Production function}
  \begin{itemize}
    \item We can remain agnostic as to the nature of the appropriate formula for the
          aggregation of outputs and use as many different products as desired.
    \item When input prices have low variability. Conditional factor demand can be
          estimated without information on input prices. Even if we add prices, due a
          lack of variability, the price parameters will be poorly estimated.

    \item From $x_i = x_i(p_1, p_2, \ldots, p_n, y_1, y_2, \ldots, y_m)$, we do not need
          to observe a complete list of inputs. But we do need to observe all input
          prices (can be ignored if almost no variability) and all outputs. While in the
          production function, it is the other way around (need to observe all inputs).
          Since, in our case, output is more *observable* than input (because capital is
          not easily observed), this approach is preferred.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{First glance}
  \begin{table}
    \fontsize{8pt}{8pt}\selectfont
    \begingroup
    \centering
    \begin{tabular}{lcc}
      \tabularnewline \midrule \midrule
      Dependent Variable: & \multicolumn{2}{c}{Nurses}                         \\
                          & OLS                        & Lagged IV             \\
      Model:              & (1)                        & (2)                   \\
      \midrule
      \emph{Variables}                                                         \\
      Constant            & 1.59$^{***}$               & 1.58$^{***}$          \\
                          & (0.067)                    & (0.069)               \\
      STAC inpatient      & 0.278$^{***}$              & 0.277$^{***}$         \\
                          & (0.012)                    & (0.013)               \\
      $\cdots$            & $\cdots$                   & $\cdots$              \\
      Private Forprofit   & -0.303$^{***}$             & -0.280$^{***}$        \\
                          & (0.061)                    & (0.065)               \\
      Private Nonprofit   & -0.215$^{***}$             & -0.188$^{***}$        \\
                          & (0.056)                    & (0.055)               \\
      Teaching            & 0.717$^{***}$              & 0.709$^{***}$         \\
                          & (0.056)                    & (0.056)               \\
      \midrule
      \emph{Fit statistics}                                                    \\
      Observations        & 15,335                     & 13,402                \\
      R$^2$               & 0.835                      & 0.837                 \\
      \midrule \midrule
      \multicolumn{3}{l}{\emph{Clustered (FI) standard-errors in parentheses}} \\
      \multicolumn{3}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}    \\
    \end{tabular}
    \par\endgroup
  \end{table}
\end{frame}

\begin{frame}
  \frametitle{Panel data Estimator}
  \begin{itemize}\itemsep=12pt
    \item Strict exogeneity: Within Group/First Diffrence
          \begin{equation*}
            E[\epsilon_{it}|x_{i1},\ldots, x_{iT},\theta_i]=0
          \end{equation*}
    \item Relaxed: First Difference GMM \citep{arellano1991some}, System GMM
          \citep{arellano1995another,blundell1998initial}.
          \begin{equation*}
            E[\epsilon_{it}|x_{i1},\ldots, x_{it-p},\theta_i]=0
          \end{equation*}
  \end{itemize}

  Issues: Weak instruments \citep{blundell_bond_1998} and the proliferation of
  instruments \citep{roodman2007short}.
  \begin{equation*}
    \E[\Delta x_{i,t-1}(y_{it}-\beta x_{it})] \quad \text{if}\quad \E\bra{\Delta x_{i,t-1}(\theta_i+\varepsilon_{i,t})}=0
  \end{equation*}
\end{frame}

\begin{frame}[label=normality]{Normality assumption on $\varepsilon_{it}$}
  Estimate the fixed effect $\theta_i$ by
  \begin{align*}
    \hat{\theta}_i =                       & \frac{1}{T}\sum(\theta_i+\varepsilon_{it}+x_{it}(\beta-\hat{\beta})) \\
    \overset{N\to \infty}{\longrightarrow} & \theta_i+\frac{1}{T}\sum_t \varepsilon_{it}
  \end{align*}
  When $T$ is relatively small (or even fixed), can't use central limit theorem to claim that $\hat{\theta}_i \overset{d}{\to} \caln(\theta_i,\frac{\sigma_i^2}{T})$.
  $\longrightarrow$ Assume that $\varepsilon_{it} \sim \caln(0,\sigma_i^2)$ .
  \hyperlink{observation}{\beamergotobutton{Back (main)}}   \hyperlink{limitation}{\beamergotobutton{Back (end)}}
\end{frame}

\begin{frame}[label=tpcontour]{TP vs PM}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Contour_Left_0.2_0.2_TPKWs_PMKWs.pdf}
  \end{figure}
  \hyperlink{tpselect}{\beamergotobutton{Back}}
\end{frame}

\begin{frame}{TP vs JS}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Contour_Left_0.2_0.2_TPKWs_JS.pdf}
  \end{figure}
\end{frame}

\begin{frame}{TP vs MLE}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{../../Figures/2013-2022/GMM_m/GLmix/Contour_Left_0.2_0.2_TPKWs_MLE.pdf}
  \end{figure}
\end{frame}

\end{document}

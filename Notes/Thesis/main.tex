\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage[small,bf]{caption}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{bbm} % for the indicator function to look good
\usepackage{color}
\usepackage{mathtools}
\usepackage{fancyhdr} % for the header
\usepackage{booktabs} % for regression table display (toprule, midrule, bottomrule)
\usepackage{adjustbox} % for regression table display
\input newcommand.tex
\bibliographystyle{apalike}
% \setlength{\parindent}{0pt} % remove the automatic indentation

\title{Something interesting}
\author{Fu Zixuan\thanks{Last compiled on \today}}
\date{July 4, 2024}
\begin{document}
\maketitle

\begin{abstract}
    \noindent  someting interesting\\

    % \noindent\textbf{Keywords:} \\

    \bigskip
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

It is almost of human nature to compare, rank and select. And competition, be
it good or bad, emerges in the wake. As invidious as ranking and selection can
be, in many cases it is one of the driving forces behind improvement in
performances, not to mention the role of natural selection in the history of
evolution. The society itself is constantly constructing league table. It
rewards the meritorious and question or even punishes the unsatisfactory. The
measure based on which rank is constructed ranges from teacher's evaluation to
communities' mobility index.

The present article extends the practice to the health sectors. To be more
specific, it studies the labor efficiency across all hospitals in France. By
exploring a comprehensive database (SAE) of French hospitals, we first
construct a measure of labor efficiency. Then based on the estimates, we
compare the public and private hospitals by selecting the top-performing units.
We borrow from the recent developments in Empirical Bayes method to achieve the
comparison.

\textbf{Conclusion here}

The article bridges two fields of interests. One is on productivity analysis.
The most popular methods are Data Envelopment Analysis
\cite{charnes1978measuring} and Stochastic Frontier Analysis
\cite{aigner1977formulation,meeusen1977efficiency}. Yet we abstract from both
of them for the convenience of statistical inference. We use the simple method
applied in \cite{croiset2024hospitals} by estimating a conditional input demand
function. To put it simply, we estimate a linear function of how much labor
input is needed to produce a give list of outputs \footnote{We refer the
    audience to xxx for detailed reasons of adopting this approach.}. We only focus
on the employment level of nurses for (reasons) \footnote{} in the
specification.

The second area of interests is the Empirical Bayes Methods. The name
"Empirical Bayes" is self-telling in the sense that Bayes implies a prior
distribution while Empirical means empirically estimate the prior from the
data. Details on how it can be of use in ranking and selection will be
presented in the rest of the section.

In \cite{croiset2024hospitals}, the authors argue that public hospital is less
efficient than private counterpart in the sense that it would need a smaller
size of personnel if it were to use the input demand function of the private
hospital, which is the main result of their counterfactuals.

Having roughly replicated the results after doubling the length of the panel,
the paper differentiates itself by including/adding the standard/classical
panel data methods in demand function estimation, specifically the fixed-effect
within-group estimation and GMM
\cite{arellano1991some,arellano1995another,blundell1998initial}.

\textbf{A tiny bit about the literature of GMM}

Though the original focus of the panel data estimator is on the $\beta$
parameters. It also provides us with a noisy estimate of the underlying
unobserved heterogeneity term denoted as $\theta_i$. (It's important to note
that this heterogeneity is not necessarily indicative of inefficiency). Now
that we have set the stage for empirical bayes, it is time to bring about the
prior distribution of $\theta_i$, denoted as $G_{\theta}$. If the prior
distribution $G$ is known, having observed an estimate $\hat{\theta}_i$ of
$\theta_i$, we can update our noisy estimate $\hat{\theta}_i$ using or
incorporating our knowledge of $G$.

The usefulness of a prior $G$ is further exemplified/highlighted in the ranking
and selection problem mentioned above, when the object of interests is the
noisy estimate of $\theta_i$. For example, in \cite{gu2023invidious}, we are
given the task of selecting the top 20\% out of the population of $\alpha_i$,
that is to say selecting those $i$ whose $alpha_i>G^{-1}(0.8)$, while
controlling for the overall false discovery rate ($\frac{x}{y}$) at 5\%. In
\cite{gu2023invidious}, the authors try to develop an optimal decision rule for
the given task. To put it in the language of optimization, they want to have a
decision rule that optimizes the performance of selection, equivalently,
minimize the loss of selection
\begin{equation*}
    \delta^* = \min_{\delta} \text{Loss} \quad \text{subject to contstraints}
\end{equation*}
where the loss function can take different forms, for example the
expected number of total type 1 and 2 mistakes.

The task at hand falls naturally under the compound decision framework
pioneered by \cite{robbins1992empirical} if we define the loss function in such
a way that takes into account the results of all the individual decisions
$\delta(Y_i)$. For instance, summing all mistakes would be one way to
aggregate/compound individual decisions,

It is obvious that in order to impose the two stated constraints (capacity and
FDR) in formulating the optimization problem, we need to know the prior
distribution $G$. Despite the importance of the $G$, it does not fall from
heaven. Therefore, Empirical Bayes methods come to the rescue, as its name
suggests, we will have to empirically estimating the unknown prior $G$.

Often times "empirically estimating \(G\)" is done with parametric assumption
that \(G\) is normal. Notable use cases are found in teacher evaluation
\cite{chetty2014measuring}, social mobility in communities
\cite{chetty2018impacts} and job discrimination \cite{kline2021reasonable}. By
assuming a Gaussian $G$, they shrink the estimated fixed effect linearly, thus
giving the name "linear shrinkage". However, departure from normality may
render the linear shrinkage rule as unhelpful. Thanks to the foundational work
of \cite{} who has shown that non-parametric estimation is also feasible and
consistent, it is preferable to relax the normality assumption and estimate the
prior $G$ non-parametrically. In terms of computation method, \cite{} has
formulated the non-parmetric estimation as a convex optimization problem.
Compared to other popular methods such as EM algorithm
\cite{laird1978nonparametric}, recent advancements in convex optimization
computation methods \cite{andersen2010mosek} has made the novel approach of
\cite{koenker2014convex} computationally more attractive.

It is worth mentioning here that though a discrete $G$ with at most x atoms can
be estimated using the REBayes package \cite{koenker2017rebayes}, we are not
free of imposing any assumptions, that is the distribution of estimate
$\hat{\alpha}_i|\alpha_i,\sigma_i^2.$ To illustrate, in the case of the
estimate of fixed effect, we have

\begin{align*}
    \hat{\alpha}_i & =\frac{1}{T}\sum_t (y_{it}-x_{it}\hat{\beta})                         \\
                   & =\frac{1}{T}\sum(\alpha_i+\varepsilon_{it}+x_{it}(\beta-\hat{\beta})) \\
                   & \to^d \alpha_i+\frac{1}{T}\sum_t \varepsilon_{it}                     \\
\end{align*}

The asymptotic distribution follows from the consistency of $\hat{\beta}$ when
$N \to \infty$, a reasonable assumption in wide panels.

If we may boldly assume that the errors are $i.i.d.$ normally distributed for
each $i$
\begin{equation*}
    \varepsilon_{it} \sim N(0, \sigma_i^2)
\end{equation*}
Then a fixed/small $T$ won't do jeopardize/imperil xxx our argument too much since we do not need to invoke central
limit theorem to have
\begin{equation*}
    \hat{\alpha}_i\to^d N(\alpha_i,\sigma_i^2/T)
\end{equation*}
However without the normality assumption on the error terms, we have to resort to the
central limit theorem from the claim that $T\to \infty$, which may seem unrealistic for a wide panel ($N>>T$).

The rest of the paper is organized as follows. Section 2 briefly describes the
data and lays out the reduced form estimation of the input demand function,
treating the number of nurses as the dependent variable and a list of 9 output
measures as the regressors. Section 3 applies the classical panel data
estimators to the same specification, distinguishing between whether strict
exogeneity is assumed. In section 4, we introduce the compound decision
framework and specifically/xxx define the selection problem. Section 5 follows
with a comparison of the different selection outcome as a result of imposing
varying constraints and assumptions. In section 6, we try to draw preliminary
conclusion on the comparative performance of public and private hospitals.
Section 7 discusses potential issues and concludes.

\newpage
\bibliography{ref.bib}

\end{document}
